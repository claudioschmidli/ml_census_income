{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val train f1: 0.7234510027260497\n",
      "Overall train f1:  0.7448500406987666\n",
      "Overall test f1:  0.7214786488209051\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns \n",
    "from seaborn import load_dataset\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load imbalanced-learn library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "#Import model library\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Load sklearn libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#Load my custom transformers\n",
    "from modules.preprocess import Drop_Columns\n",
    "from modules.preprocess import InteractionsTransformer\n",
    "from modules.preprocess import OrderFeatures\n",
    "from modules.preprocess import Impute_Missing\n",
    "from modules.preprocess import Predict_Missing\n",
    "from modules.preprocess import drop_rows\n",
    "\n",
    "\n",
    "columns = [\n",
    " 'age',\n",
    " 'workclass',\n",
    " 'fnlwgt',\n",
    " 'education',\n",
    " 'educational num',\n",
    " 'marital',\n",
    " 'occupation',\n",
    " 'relationship',\n",
    " 'race',\n",
    " 'gender',\n",
    " 'capital gain',\n",
    " 'capital loss',\n",
    " 'hours per week',\n",
    " 'country',\n",
    " 'income'\n",
    " ]\n",
    "\n",
    "#Load the dataset\n",
    "train = pd.read_csv('data/adult.data', sep=\",\", names=columns)\n",
    "test = pd.read_csv('data/adult.test', sep=\",\", names=columns)\n",
    "\n",
    "#Drop missing\n",
    "train = drop_rows(train, ['workclass', 'occupation', 'country'], drop_val =' ?')\n",
    "test = drop_rows(test, ['workclass', 'occupation', 'country'], drop_val =' ?')\n",
    "\n",
    "#preprocess\n",
    "train['income'] = train['income'].str.strip(' .')\n",
    "test['income'] = test['income'].str.strip(' .')\n",
    "\n",
    "#Define and encode labels\n",
    "train['income'].replace({'<=50K':0, '>50K':1}, inplace=True)\n",
    "test['income'].replace({'<=50K':0, '>50K':1}, inplace=True)\n",
    "y_train=train['income']\n",
    "y_test=test['income']\n",
    "\n",
    "#Define features and labels, the column education is already covered with the ordinal feature education.num\n",
    "X_train = train.drop(['income'], axis='columns')\n",
    "X_test = test.drop(['income'], axis='columns')\n",
    "\n",
    "#Define model\n",
    "model = lgb.LGBMClassifier(random_state=0)#class_weight={1:3, 0:1}\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (StandardScaler(), ['age', 'educational num', 'capital gain', 'capital loss', 'hours per week']), #dont forget fnlwgt if in dataset!\n",
    "    (OrdinalEncoder(), ['gender']),\n",
    "    (OneHotEncoder(handle_unknown='ignore', sparse=False), ['workclass', 'marital', 'occupation', 'relationship']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "#Compose pipeline\n",
    "pipe = imbpipeline(steps = [\n",
    "        ['orderfeatures', OrderFeatures()],\n",
    "        ['drop_columns', Drop_Columns(['education', 'fnlwgt', 'race', 'country'])],\n",
    "        #('impute_missing', Impute_Missing(missing_value=' ?', pred_columns=['workclass', 'occupation', 'country'], strategy = 'most_frequent' , label_missing=False)),\n",
    "        #['predict_missing', Predict_Missing(pred_columns=['workclass', 'occupation', 'country'], print_cross_val_score=False, label_missing = False)],\n",
    "        ['transformer', transformer],\n",
    "        #['interactions', InteractionsTransformer(use_cache=False)],  \n",
    "        ['smote', SMOTE(random_state=0)],\n",
    "        ['model', model]\n",
    "]\n",
    ")\n",
    "\n",
    "#Calculate scores\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0,  shuffle=True)\n",
    "\n",
    "#Apply pipeline on data\n",
    "pipe.fit(X_train, y_train)\n",
    "y_test_predicted = pipe.predict(X_test)\n",
    "y_train_predicted = pipe.predict(X_train)\n",
    "\n",
    "print(\"Cross val train f1:\", cross_val_score(pipe, X_train, y_train, cv=skf, scoring='f1').mean())\n",
    "print(\"Overall train f1: \", get_scorer('f1')._score_func(y_train, y_train_predicted))\n",
    "print(\"Overall test f1: \", get_scorer('f1')._score_func(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of lightgbm model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I shortly discuss the approaches to improve the f1 score of the lightgbm model. To reproduce the results you just have to uncomment the transformers and functions in the cell above. For each step I describe which transformers were used.\n",
    "\n",
    "Note: While optinizing the model, only the cross validation score on the training set was considered. The score on the test set was calculated at the very end of the project!\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>How?</th>\n",
    "    <th>Results</th>\n",
    "    <th>Conclusion</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Base model</td>\n",
    "    <td>Model<br>lgb.LGBMClassifier(random_state=0)<br><br>Transformers<br>(StandardScaler(), ['age', ..]),<br>(OrdinalEncoder(), ['gender']),<br>(OneHotEncoder(handle_unknown='ignore', sparse=False), ['workclass', ..]  <br>OrderFeatures()<br>Drop_Columns(['education', 'fnlwgt'])<br><br><br>(have a look at the code above to see all parameters of the transformers)</td>\n",
    "    <td><br><b>Cross val train f1: 0.71776</b><br>(Overall train f1:  0.74101)<br>(Overall test f1:  0.71205)</td>\n",
    "    <td>I already dropped the column education in the base model because it is redundant.<br>Education_num contains the same information but is in a ordered numeric format.<br><br>Additionally, I dropped the column fnlwgt because it does not contain <br>relevant information for improving the f1 score. <br>However, one can think about weighing the observations with the fnlwgt values. <br>I don't do this here.<br>The goal of this ML project is just to train a model with high f1 score.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Drop missing values</td>\n",
    "    <td>I made two functions to do this:<br><br>drop_rows(train, ['workclass', 'occupation', 'country'], drop_val =' ?')<br>drop_rows(test,  ['workclass', 'occupation', 'country'], drop_val =' ?')</td>\n",
    "    <td><b>Cross val train f1: 0.71884</b><br>(Overall train f1:  0.745140)<br>(Overall test f1:  0.71326)<br></td>\n",
    "    <td>Better score when dropping rows with missing values.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Impute missing</td>\n",
    "    <td>I created a custom transformer to do this:<br><br>Impute_Missing(... strategy = 'most_frequent' , label_missing = False)<br><br>Of course the fitting is only done on the training data! We never wanna have data leakage!</td>\n",
    "    <td><b>Cross val train f1: 0.71569</b><br>(Overall train f1:  0.73912)<br>(Overall test f1:  0.70599)<br><br></td>\n",
    "    <td>Imputing with most frequent value is worse than dropping rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Impute missing and <br>label imputed features</td>\n",
    "    <td>I created a custom transformer to do this:<br><br>Impute_Missing(... strategy = 'most_frequent' , label_missing = True)<br><br>Of course the fitting is only done on the training data! We never wanna have data leakage!</td>\n",
    "    <td><b>Cross val train f1: 0.71570</b><br>(Overall train f1:  0.73902)<br>(Overall test f1:  0.709732)<br></td>\n",
    "    <td>Adding label to imputed values helps but score is still worse.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Predict missing values<br>by ML</td>\n",
    "    <td>I created a custom transformer to do this. Of course I fitted the model only to the training data!  <br>We never want data leakage!<br><br>Predict_Missing(pred_columns=['workclass', 'occupation', 'country'], .., label_missing = False)<br><br>Of course the fitting is only done on the training data! We never wanna have data leakage!</td>\n",
    "    <td><b>Cross val train f1: 0.71552</b><br>(Overall train f1:  0.73953)<br>(Overall test f1:  0.70722)<br></td>\n",
    "    <td>Predicting values with ML is worse than dropping rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Predict missing values<br>by ML and label <br>predicted values</td>\n",
    "    <td>I created a custom transformer to do this:<br><br>Predict_Missing(pred_columns=['workclass', 'occupation', 'country'], .., label_missing = True)<br><br>Of course the fitting is only done on the training data! We never wanna have data leakage!</td>\n",
    "    <td><b>Cross val train f1: 0.71795</b><br>(Overall train f1:  0.74126)<br>(Overall test f1:  0.70733)<br></td>\n",
    "    <td>Adding label to predicted values helps but score is still worse.<br><br>=&gt; we will drop rows with missing values for subsequent steps!</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Oversampling <br>with class weights</td>\n",
    "    <td>LogisticRegression(.., class_weight={1:3, 0:1})<br>(see code above for all parameters)</td>\n",
    "    <td><b>Cross val train f1: 0.72066</b><br>(Overall train f1:  0.74377)<br>(Overall test f1:  0.71726)<br></td>\n",
    "    <td>Using&nbsp;&nbsp;class weights significantly improves the score.<br><br><br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Oversampling <br>with SMOTE</td>\n",
    "    <td>SMOTE(random_state=0)<br><br>Of course the oversampling is only done on the training data and withing the CV steps! <br>We never wanna have data leakage!<br>For this purpose I used imbpipeline <br>from <a href=\"https://imbalanced-learn.org/stable/\">imbalanced-learn</a><br></td>\n",
    "    <td><b>Cross val train f1: 0.72175</b><br>(Overall train f1:  0.74162)<br>(Overall test f1:  0.71813)<br><br><br></td>\n",
    "    <td>Oversampling the minor class significantly improves the score.<br>SMOTE leads to a slightly better score than the class weights.<br>I will thus use SMOTE to handle the imbalanced targets for the next steps.<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Drop columns with low <br>permutation importance</td>\n",
    "    <td>According to the calculated feature importance <br>race and country contain at least relevant information.<br>(Note: other columns were tested as well but the last two ones are documented here)<br><br>Drop_Columns(['education', 'fnlwgt', 'race', 'country'])</td>\n",
    "    <td>Drop Country:<br><b>Cross val train f1: 0.72477</b><br>Overall train f1:  0.74273<br>Overall test f1:  0.71930<br><br><br>Drop Race:<br><b>Cross val train f1: 0.72335</b><br>Overall train f1:  0.74155<br>Overall test f1:  0.71976<br><br>Drop Country and Race<br><b>Cross val train f1: 0.72345</b><br>Overall train f1:  0.74485<br>Overall test f1:  0.72147<br><br></td>\n",
    "    <td>Race and Country were determined to have the lowest feature importance <br>(see <a href=\"Feature_Importance.ipynb\">Feature_Importance.ipynb</a>).<br><br>When dropping country these features I observed a better f1 score. <br><br><br>Thus, I only excluded these features for the proceeding steps.<br><br>Note:<br>I also tested dropping other features with low permutation importance <br>but with other ones I could not increase the f1 score <br>(fnlwgt and education were already excluded in the base model!).<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Add interactions</td>\n",
    "    <td>I made a custom transformer to do this. Of course I fitted the model only to the training data!  <br>We never want data leakage!<br><br>InteractionsTransformer()<br>Of course the fitting is only done on the training data! We never wanna have data leakage!</td>\n",
    "    <td><b>Cross val train f1: 0.72691</b><br>(Overall train f1:  0.75141)<br>(Overall test f1:  0.72104)<br></td>\n",
    "    <td>Decision tree based models can learn interactions themselves through recursive splitting. <br>However, I still could increase the f1 score by creating manual features. <br>I will not use it for subsequent steps. </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hyperparameter tuning</td>\n",
    "    <td>Because not all penalties work with all solvers I had to do several rounds of grid search.<br><br>Furthermore, its better to start with a larger parameter space.<br>In this way I can narrowing down to the optimal values.<br><br>Round 1<br>'model__num_leaves':[10,31, 100],<br>'model__min_child_samples':[5, 20, 40],<br>'model__max_depth':[2, 10, 100],<br>'model__learning_rate':[0.05, 0.1, 0.4],  <br>'model__reg_alpha':[0, 0.01, 0.1],<br>'model__reg_lambda': [0, 0.01, 0.1]<br><br><br>Round 2<br>'model__num_leaves':[80,100, 120],<br>'model__min_child_samples':[10, 15, 20],<br>'model__max_depth':[2, 5, 7],<br>'model__learning_rate':[0.2, 0.3, 0.4],  <br>'model__reg_alpha':[0.02, 0.03, 0.04],<br>'model__reg_lambda': [0, 0.05, 0.1]<br></td>\n",
    "    <td><b>Cross val train f1: 0.72795</b><br>(Overall train f1:  0.75872<br>(Overall test f1:  0.71947)</td>\n",
    "    <td>By tuning the hyperparameters I could again slightly improve the score.<br><br><br>Note: Due to limited computational power and time I could not check to many parameters.</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudio/Documents/DAS/Advanced_ML/Project/pyenv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7212035252940138\n",
      "{'model__learning_rate': 0.2, 'model__max_depth': 5, 'model__min_child_samples': 10, 'model__num_leaves': 70, 'model__reg_alpha': 0.03, 'model__reg_lambda': 0.02}\n",
      "Pipeline(steps=[('orderfeatures', OrderFeatures()),\n",
      "                ('drop_columns',\n",
      "                 Drop_Columns(columns=['education', 'fnlwgt', 'race',\n",
      "                                       'country'])),\n",
      "                ('transformer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('standardscaler',\n",
      "                                                  StandardScaler(),\n",
      "                                                  ['age', 'educational num',\n",
      "                                                   'capital gain',\n",
      "                                                   'capital loss',\n",
      "                                                   'hours per week']),\n",
      "                                                 ('ordinalencoder',\n",
      "                                                  OrdinalEncoder(),\n",
      "                                                  ['gender']),\n",
      "                                                 ('onehotencoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse=False),\n",
      "                                                  ['workclass', 'marital',\n",
      "                                                   'occupation',\n",
      "                                                   'relationship'])])),\n",
      "                ('interactions', InteractionsTransformer()),\n",
      "                ('smote', SMOTE(random_state=0)),\n",
      "                ['model',\n",
      "                 LGBMClassifier(learning_rate=0.2, max_depth=5,\n",
      "                                min_child_samples=10, num_leaves=70,\n",
      "                                random_state=0, reg_alpha=0.03,\n",
      "                                reg_lambda=0.02)]])\n"
     ]
    }
   ],
   "source": [
    "y_train=train['income']\n",
    "y_test=test['income']\n",
    "\n",
    "#Define features and labels, the column education is already covered with the ordinal feature education.num\n",
    "X_train = train.drop(['income'], axis='columns')\n",
    "X_test = test.drop(['income'], axis='columns')\n",
    "\n",
    "#Define model\n",
    "model = lgb.LGBMClassifier(random_state=0)#class_weight={1:3, 0:1}\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (StandardScaler(), ['age', 'educational num', 'capital gain', 'capital loss', 'hours per week']), #dont forget fnlwgt if in dataset!\n",
    "    (OrdinalEncoder(), ['gender']),\n",
    "    (OneHotEncoder(handle_unknown='ignore', sparse=False), ['workclass', 'marital', 'occupation', 'relationship']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "#Compose pipeline\n",
    "pipe = imbpipeline(steps = [\n",
    "        ['orderfeatures', OrderFeatures()],\n",
    "        ['drop_columns', Drop_Columns(['education', 'fnlwgt', 'race', 'country'])],\n",
    "        #('impute_missing', Impute_Missing(missing_value=' ?', pred_columns=['workclass', 'occupation', 'country'], strategy = 'most_frequent' , label_missing=False)),\n",
    "        #['predict_missing', Predict_Missing(pred_columns=['workclass', 'occupation', 'country'], print_cross_val_score=False, label_missing = False)],\n",
    "        ['transformer', transformer],\n",
    "        #['interactions', InteractionsTransformer(use_cache=False)],  \n",
    "        ['smote', SMOTE(random_state=0)],\n",
    "        ['model', model]\n",
    "]\n",
    ")\n",
    "\n",
    "#Calculate scores\n",
    "skf = StratifiedKFold(n_splits=2, random_state=0,  shuffle=True)\n",
    "\n",
    "pipe = GridSearchCV(\n",
    "        pipe,\n",
    "            param_grid={\n",
    "\n",
    "                        'model__num_leaves':[80,100, 120],\n",
    "                        'model__min_child_samples':[10, 15, 20],\n",
    "                        'model__max_depth':[2, 5, 7],\n",
    "                        'model__learning_rate':[0.2, 0.3, 0.4],  # 10**(np.linspace(-2, 0, 3))\n",
    "                        'model__reg_alpha':[0.02, 0.03, 0.04],\n",
    "                        'model__reg_lambda': [0, 0.05, 0.1]\n",
    "                        #'model__subsample': [0.8, 1], \n",
    "                        #'model__colsample_bytree': [1, 0.9]\n",
    "                        #'model__subsample': [0.8]\n",
    "\n",
    "}\n",
    ",\n",
    "         cv=skf, refit=True, scoring = 'f1'\n",
    ")\n",
    "\n",
    "#Apply pipeline on data\n",
    "pipe.fit(X_train, y_train)\n",
    "y_test_predicted = pipe.predict(X_test)\n",
    "y_train_predicted = pipe.predict(X_train)\n",
    "\n",
    "print(pipe.best_score_)\n",
    "print(pipe.best_params_)\n",
    "print(pipe.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val train f1: 0.7279547575038213\n",
      "Overall train f1:  0.7587253414264036\n",
      "Overall test f1:  0.7194761843625627\n"
     ]
    }
   ],
   "source": [
    "y_train=train['income']\n",
    "y_test=test['income']\n",
    "\n",
    "#Define features and labels, the column education is already covered with the ordinal feature education.num\n",
    "X_train = train.drop(['income'], axis='columns')\n",
    "X_test = test.drop(['income'], axis='columns')\n",
    "\n",
    "#Define model\n",
    "model = lgb.LGBMClassifier(random_state=0, learning_rate = 0.3, max_depth = 5, min_child_samples= 15, num_leaves= 80, reg_alpha= 0.03, reg_lambda = 0)\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (StandardScaler(), ['age', 'educational num', 'capital gain', 'capital loss', 'hours per week']), #dont forget fnlwgt if in dataset!\n",
    "    (OrdinalEncoder(), ['gender']),\n",
    "    (OneHotEncoder(handle_unknown='ignore', sparse=False), ['workclass', 'marital', 'occupation', 'relationship']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "\n",
    "#Compose pipeline\n",
    "pipe = imbpipeline(steps = [\n",
    "        ['orderfeatures', OrderFeatures()],\n",
    "        ['drop_columns', Drop_Columns(['education', 'fnlwgt', 'race', 'country'])],\n",
    "        #('impute_missing', Impute_Missing(missing_value=' ?', pred_columns=['workclass', 'occupation', 'country'], strategy = 'most_frequent' , label_missing=False)),\n",
    "        #['predict_missing', Predict_Missing(pred_columns=['workclass', 'occupation', 'country'], print_cross_val_score=False, label_missing = False)],\n",
    "        ['transformer', transformer],\n",
    "        #['interactions', InteractionsTransformer(use_cache=False)],  \n",
    "        ['smote', SMOTE(random_state=0)],\n",
    "        ['model', model]\n",
    "]\n",
    ")\n",
    "\n",
    "#Calculate scores\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0,  shuffle=True)\n",
    "\n",
    "#Apply pipeline on data\n",
    "pipe.fit(X_train, y_train)\n",
    "y_test_predicted = pipe.predict(X_test)\n",
    "y_train_predicted = pipe.predict(X_train)\n",
    "\n",
    "print(\"Cross val train f1:\", cross_val_score(pipe, X_train, y_train, cv=skf, scoring='f1').mean())\n",
    "print(\"Overall train f1: \", get_scorer('f1')._score_func(y_train, y_train_predicted))\n",
    "print(\"Overall test f1: \", get_scorer('f1')._score_func(y_test, y_test_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "900aa3c88b9e523c15bd03ed94e3ffd6c7de8c1f22936e93006d5eaf906f882d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
